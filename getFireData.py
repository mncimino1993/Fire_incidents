# coding: utf-8

# # Example 01: Basic Queries
#
# Retrieving data from Socrata databases using sodapy

# ## Setup

# In[1]:


import os
import pandas as pd
import numpy as np
from sodapy import Socrata
from sqlalchemy import create_engine
import psycopg2 


# ## Find some data
#
# Though any organization can host their own data with Socrata's tools, Socrata also hosts several open datasets themselves:
#
# https://opendata.socrata.com/browse
#
# The following search options can help you find some great datasets for getting started:
# * Limit to data sets (pre-analyzed stuff is great, but if you're using sodapy you probably want the raw numbers!)
# * Sort by "Most Accessed"
#
# [Here's](https://opendata.socrata.com/browse?limitTo=datasets&sortBy=most_accessed&utf8=%E2%9C%93&page=1) a link that applies those filters automatically.
#
# Click on a few listings until you find one that looks interesting. Then click API and extract the following bits of data from the displayed url.
#
# https://<**opendata.socrata.com**>/dataset/Santa-Fe-Contributors/<**f92i-ik66**>.json
#

# ![Socrata Interface](socrata_interface.png)

# In[4]:


# Enter the information from those sections here
socrata_domain = "data.sfgov.org"
socrata_dataset_identifier = "wr8u-xric"

# App Tokens can be generated by creating an account at https://opendata.socrata.com/signup
# Tokens are optional (`None` can be used instead), though requests will be rate limited.
#
# If you choose to use a token, run the following command on the terminal (or add it to your .bashrc)
# $ export SODAPY_APPTOKEN=<token>
socrata_token = None


# ## Get all the data

# In[5]:


client = Socrata(socrata_domain, socrata_token)



# In[6]:

engine = create_engine('postgresql://pgadmin:thisIsASecurePassword33!@postgres:5432/postgres',pool_pre_ping=True)
start = 0
chunkSize = 20000



recordCount = client.get(socrata_dataset_identifier, select="COUNT(*)")
recordCount = int(recordCount[0]['COUNT'])

print("Records found: ",recordCount)
print("Starting to load records")
while True:
    results =[]   
    results.extend( client.get(socrata_dataset_identifier, offset=start, limit=chunkSize))
    df = pd.DataFrame.from_records(results)
    dfpoint = pd.json_normalize(df["point"])
    dfc = dfpoint["coordinates"].apply(pd.Series)
    dfc.columns = ['longitude','latitude']
    df = df.drop(columns='point')
    df = pd.concat([df,dfc],axis=1)
    if (start == 0 ):
        df.to_sql('STG_RAW', engine, if_exists='replace',index=False,method='multi',chunksize=chunkSize)
    else:
        df.to_sql('STG_RAW', engine, if_exists='append',index=False,method='multi',chunksize=chunkSize)  

    start = start + chunkSize    
    print("Loaded ",start," records of ",recordCount, "total records")    
    if (start > recordCount):
        break    

# Success! Let's do some minimal cleaning and analysis just to justify the bandwidth used.

# In[7]:




# In[8]:



# ## Multiple Data Sources
#
# That was much less annoying than downloading a CSV, though you can always save the dataframe to a CSV if you'd like. Where sodapy really shines though is in grabbing different data sources and mashing them together.
#
# For example, let's compare 311 calls between [New York City](https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9) and [Chattanooga, TN](https://data.chattlibrary.org/Government/311-Service-Requests/9iep-6yhz). Socrata makes it so easy, you'd be crazy _not_ to do it!


# Looks like trees are a higher percentage of NYC complaints than Chattanooga's.
#
# Note that we can only talk about percentages, since our query results got truncated to 1,000 rows.
#
# What if we want to be smarter about what we ask for, so that we can get 100% of the subset of data
# we're most interested in? That's the subject of a future example, so stay tuned!
#
# If you want to find more data sets, here's Socrata's data finder:
#
# https://www.opendatanetwork.com/search